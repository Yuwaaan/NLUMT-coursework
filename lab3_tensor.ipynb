{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b74a23d9",
   "metadata": {},
   "source": [
    "# NLU+ 2021-2022 Lab 3: Tensor Computation in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476f4fee",
   "metadata": {},
   "source": [
    "#### Authors: Yao Fu and Frank Keller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3478970",
   "metadata": {},
   "source": [
    "This lab indends to teach computation with tensors, which is a fundamental paradigm in modern machine learning. \n",
    "\n",
    "Students should work through Section 1 before the lab session, as this section introduces the basics of tensor computation. The lab session will then focus on more advanced computations with tensors, in Sections 2 and 3.\n",
    "\n",
    "**Students are strongly encouraged to complete this lab before starting CW2. Many computations in the lab will be encountered in CW2 again, so the amount of difficult in the coursework will be significantly reduced.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e8eb80",
   "metadata": {},
   "source": [
    "We suggest using jupyter lab instead of jupyter notebook. Basically jupyter lab is an enhanced version of jupyter notebook. Downloading jupyter lab is simply:\n",
    "\n",
    "```bash\n",
    "pip install jupyterlab\n",
    "```\n",
    "\n",
    "Then in a terminal, start jupyter lab with \n",
    "```bash\n",
    "jupyter lab\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e9f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb23019",
   "metadata": {},
   "source": [
    "# Section 1. Basic Tensor Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc74517",
   "metadata": {},
   "source": [
    "## Definition of Tensor: High-dimensional Array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe659e7",
   "metadata": {},
   "source": [
    "### Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f145a9",
   "metadata": {},
   "source": [
    "A vector is a rank-1 (or one-dimentional) tensor.\n",
    "\n",
    "In NLP, it could be a single sentence where each word is an integer index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7960fb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = torch.tensor([1, 3, 5, 7, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3325a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5, 7, 9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a23c4",
   "metadata": {},
   "source": [
    "### Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238c9642",
   "metadata": {},
   "source": [
    "A matrix is a rank-2 (or two-dimentional) tensor.\n",
    "\n",
    "In NLP, this could used to represent a batch of different sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8f4c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = torch.tensor([[1, 3, 5, 7, 9], \n",
    "                     [2, 4, 6, 8, 10]]) # a batch of two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdc657dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  3,  5,  7,  9],\n",
       "        [ 2,  4,  6,  8, 10]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents # we assume these index corresponds to some actual words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8d5482",
   "metadata": {},
   "source": [
    "### Rank-3 Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5247f7c",
   "metadata": {},
   "source": [
    "There can be a rank-3 (three-dimensional) tensor.\n",
    "\n",
    "In NLP, this could be a batch of sentences, where each word in each sentence is representened as an embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34bf6ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_emb = torch.rand([2, 5, 10])  # [batch_size = 2, sentence_length = 5, hidden_size = 10]\n",
    "                                    # we use random vectors for the purpose of demonstration \n",
    "                                    # we use the name `hidden_size` because the embedding is usually referred as 'hidden representations' in neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8391ae15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5888, 0.1384, 0.3219, 0.5481, 0.8978, 0.3362, 0.5466, 0.3833,\n",
       "          0.6710, 0.8290],\n",
       "         [0.1809, 0.0470, 0.9739, 0.8530, 0.5467, 0.4967, 0.0050, 0.4975,\n",
       "          0.1456, 0.0031],\n",
       "         [0.1117, 0.2034, 0.1183, 0.9997, 0.9059, 0.7601, 0.3395, 0.5432,\n",
       "          0.7731, 0.3103],\n",
       "         [0.7531, 0.3300, 0.7945, 0.8864, 0.4057, 0.4517, 0.6010, 0.8064,\n",
       "          0.3448, 0.6773],\n",
       "         [0.3902, 0.9508, 0.0760, 0.8834, 0.9523, 0.4662, 0.0296, 0.1755,\n",
       "          0.9334, 0.5587]],\n",
       "\n",
       "        [[0.1730, 0.7177, 0.2852, 0.0100, 0.7166, 0.9565, 0.7168, 0.0939,\n",
       "          0.6218, 0.2003],\n",
       "         [0.4821, 0.5393, 0.0591, 0.4795, 0.4916, 0.5234, 0.5107, 0.3697,\n",
       "          0.6168, 0.5117],\n",
       "         [0.4011, 0.3225, 0.3118, 0.0167, 0.5144, 0.0053, 0.8730, 0.0512,\n",
       "          0.1928, 0.0469],\n",
       "         [0.4480, 0.2642, 0.2911, 0.6106, 0.7011, 0.2939, 0.1632, 0.4831,\n",
       "          0.9947, 0.6860],\n",
       "         [0.8765, 0.6472, 0.8652, 0.9155, 0.8116, 0.7634, 0.0687, 0.8537,\n",
       "          0.2044, 0.7790]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b5e74",
   "metadata": {},
   "source": [
    "Here `sents_emb[i, j]` means the word embedding for the j-th word in the i-th sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25172854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7531, 0.3300, 0.7945, 0.8864, 0.4057, 0.4517, 0.6010, 0.8064, 0.3448,\n",
       "        0.6773])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  word embedding for 0-th sentence, 3-rd word\n",
    "sents_emb[0, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7ceea0",
   "metadata": {},
   "source": [
    "### Rank-4 Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf09949",
   "metadata": {},
   "source": [
    "As for a rank-4 (four-dimensional) tensor, this could be a batch of sentences, where each word of each sentence is further devided into different heads of attention keys for multi-head attention (you will encounter this in CW2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07ee7ece",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.8334, 0.8729, 0.9025, 0.6452, 0.3510, 0.8613, 0.7899, 0.6674,\n",
       "           0.9150, 0.7840],\n",
       "          [0.1847, 0.5535, 0.6011, 0.5433, 0.1122, 0.1387, 0.7824, 0.3964,\n",
       "           0.5836, 0.0032],\n",
       "          [0.3672, 0.0614, 0.6729, 0.5840, 0.0016, 0.3817, 0.7127, 0.0014,\n",
       "           0.1786, 0.9084],\n",
       "          [0.8757, 0.9873, 0.1219, 0.6754, 0.9230, 0.8386, 0.8947, 0.1473,\n",
       "           0.8560, 0.3940]],\n",
       "\n",
       "         [[0.3179, 0.3308, 0.9132, 0.1591, 0.3336, 0.0413, 0.8887, 0.3742,\n",
       "           0.0330, 0.3727],\n",
       "          [0.1295, 0.3205, 0.5178, 0.5268, 0.7565, 0.5705, 0.2365, 0.8082,\n",
       "           0.8561, 0.6492],\n",
       "          [0.1867, 0.9840, 0.7758, 0.6262, 0.2417, 0.2887, 0.2965, 0.4177,\n",
       "           0.7826, 0.4871],\n",
       "          [0.6364, 0.6007, 0.5918, 0.4352, 0.0487, 0.6366, 0.1348, 0.8677,\n",
       "           0.2871, 0.1356]],\n",
       "\n",
       "         [[0.4954, 0.3594, 0.3975, 0.0518, 0.9698, 0.3253, 0.7577, 0.4650,\n",
       "           0.4150, 0.4991],\n",
       "          [0.9952, 0.9912, 0.1387, 0.1541, 0.1930, 0.5015, 0.4293, 0.3002,\n",
       "           0.4657, 0.6610],\n",
       "          [0.5574, 0.6919, 0.2448, 0.5146, 0.3169, 0.1226, 0.3471, 0.1492,\n",
       "           0.4953, 0.1999],\n",
       "          [0.6792, 0.2616, 0.2614, 0.6155, 0.7317, 0.2351, 0.5597, 0.1290,\n",
       "           0.3548, 0.1472]],\n",
       "\n",
       "         [[0.7748, 0.6291, 0.6374, 0.2383, 0.4704, 0.6025, 0.7852, 0.9046,\n",
       "           0.4481, 0.4605],\n",
       "          [0.3760, 0.9614, 0.7614, 0.9325, 0.5496, 0.3769, 0.4365, 0.7999,\n",
       "           0.4480, 0.7807],\n",
       "          [0.8757, 0.0044, 0.5574, 0.3041, 0.5696, 0.5299, 0.2368, 0.4774,\n",
       "           0.0051, 0.7699],\n",
       "          [0.7108, 0.2217, 0.4519, 0.1291, 0.3758, 0.5096, 0.4929, 0.7431,\n",
       "           0.8968, 0.4357]],\n",
       "\n",
       "         [[0.6946, 0.7394, 0.8725, 0.4858, 0.1086, 0.7844, 0.3104, 0.5699,\n",
       "           0.0870, 0.3478],\n",
       "          [0.6840, 0.8955, 0.0373, 0.8702, 0.9629, 0.6660, 0.2640, 0.5048,\n",
       "           0.6372, 0.9915],\n",
       "          [0.3574, 0.8986, 0.3639, 0.1536, 0.8419, 0.2226, 0.9963, 0.0507,\n",
       "           0.6431, 0.3495],\n",
       "          [0.8522, 0.3769, 0.7232, 0.1433, 0.5051, 0.2982, 0.4983, 0.7094,\n",
       "           0.6577, 0.4977]]],\n",
       "\n",
       "\n",
       "        [[[0.2559, 0.2559, 0.9162, 0.7375, 0.6582, 0.6377, 0.5439, 0.2733,\n",
       "           0.1401, 0.1108],\n",
       "          [0.3211, 0.0384, 0.0264, 0.1529, 0.9963, 0.0339, 0.8921, 0.0298,\n",
       "           0.7980, 0.6448],\n",
       "          [0.2794, 0.7555, 0.8420, 0.2294, 0.2386, 0.9277, 0.0665, 0.1536,\n",
       "           0.3404, 0.6123],\n",
       "          [0.6528, 0.8263, 0.8365, 0.4561, 0.7731, 0.9553, 0.1870, 0.9385,\n",
       "           0.7123, 0.9064]],\n",
       "\n",
       "         [[0.2905, 0.4046, 0.1233, 0.3721, 0.8163, 0.5609, 0.9449, 0.3421,\n",
       "           0.6740, 0.0334],\n",
       "          [0.5053, 0.4452, 0.7289, 0.3012, 0.6445, 0.5195, 0.0187, 0.9848,\n",
       "           0.5169, 0.2875],\n",
       "          [0.5468, 0.6296, 0.0364, 0.3594, 0.4154, 0.6182, 0.7581, 0.9338,\n",
       "           0.5036, 0.1573],\n",
       "          [0.7732, 0.3786, 0.6289, 0.0023, 0.0223, 0.7829, 0.8542, 0.2147,\n",
       "           0.4096, 0.2016]],\n",
       "\n",
       "         [[0.4684, 0.4712, 0.1777, 0.1419, 0.2479, 0.0469, 0.7636, 0.5502,\n",
       "           0.9655, 0.5543],\n",
       "          [0.2101, 0.1784, 0.0907, 0.3211, 0.1360, 0.0038, 0.0503, 0.9020,\n",
       "           0.9479, 0.0620],\n",
       "          [0.8120, 0.1742, 0.9005, 0.3474, 0.6063, 0.2215, 0.8284, 0.4906,\n",
       "           0.5795, 0.8834],\n",
       "          [0.1671, 0.4636, 0.8296, 0.5921, 0.2010, 0.0285, 0.5290, 0.8736,\n",
       "           0.2330, 0.2883]],\n",
       "\n",
       "         [[0.2877, 0.5907, 0.0101, 0.1147, 0.8250, 0.3939, 0.8492, 0.1400,\n",
       "           0.6998, 0.5963],\n",
       "          [0.4889, 0.1555, 0.2942, 0.2694, 0.1815, 0.8392, 0.7912, 0.6716,\n",
       "           0.9553, 0.3265],\n",
       "          [0.9506, 0.5587, 0.5181, 0.2583, 0.2951, 0.3660, 0.6668, 0.6533,\n",
       "           0.0087, 0.0979],\n",
       "          [0.2865, 0.1731, 0.9630, 0.4493, 0.1557, 0.7922, 0.4139, 0.8403,\n",
       "           0.8637, 0.6278]],\n",
       "\n",
       "         [[0.9926, 0.5008, 0.9164, 0.0555, 0.6972, 0.7832, 0.0674, 0.6843,\n",
       "           0.6759, 0.5344],\n",
       "          [0.7981, 0.4191, 0.9658, 0.8410, 0.1329, 0.7871, 0.8482, 0.5406,\n",
       "           0.9348, 0.9902],\n",
       "          [0.0250, 0.1591, 0.1167, 0.0050, 0.0480, 0.5409, 0.1943, 0.0017,\n",
       "           0.0714, 0.3833],\n",
       "          [0.0891, 0.6190, 0.7699, 0.1945, 0.1790, 0.0597, 0.2155, 0.6224,\n",
       "           0.6407, 0.9017]]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_emb_keys = torch.rand([2, 5, 4, 10])  # [batch_size = 2, sentence_length = 5, number_of_heads = 4, hidden_size = 10]\n",
    "                                            # Again, we use random vectors for the purpose of demonstration\n",
    "sents_emb_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc29d593",
   "metadata": {},
   "source": [
    "Here `sents_emb_keys[i, j, k]` means the attention key vector in the i-th sentence, j-th word, k-th head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a07472a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9587, 0.1933, 0.1780, 0.1439, 0.7586, 0.4737, 0.7456, 0.0403, 0.6641,\n",
       "        0.2561])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# key vector for 0-th, 3-rd word, 2-nd head\n",
    "sents_emb_keys[0, 3, 2] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62700f9c",
   "metadata": {},
   "source": [
    "### Caveat: always keep in mind the meaning of shapes of tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe9419a",
   "metadata": {},
   "source": [
    "It is important to always have a record of the meaning of the shape, otherwise one would quickly forget what each tensor dimension means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b8b525f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 4, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_emb_keys.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a6b6fe",
   "metadata": {},
   "source": [
    "Question: what does [2, 5, 4, 10] mean?\n",
    "\n",
    "Answer: it means [batch_size = 2, sentence_length = 5, number_of_heads = 4, hidden_size = 10], where `number_of_heads` means the number of heads used in the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695e42cf",
   "metadata": {},
   "source": [
    "## Basic Tensor Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284104c5",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "442bffa4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9654, 0.7689, 0.3381, 0.4762, 0.1286, 0.4259, 0.8224, 0.8908,\n",
       "          0.8797, 0.9354],\n",
       "         [0.4685, 0.8491, 0.3870, 0.5836, 0.8562, 0.8005, 0.2111, 0.8136,\n",
       "          0.9285, 0.4225],\n",
       "         [0.4361, 0.5648, 0.7100, 0.8261, 0.4496, 0.5599, 0.0967, 0.7952,\n",
       "          0.2972, 0.5106],\n",
       "         [0.7805, 0.6896, 0.7740, 0.6492, 0.6878, 0.2625, 0.8063, 0.2552,\n",
       "          0.0272, 0.8594]],\n",
       "\n",
       "        [[0.0183, 0.0201, 0.8848, 0.9264, 0.9478, 0.4991, 0.8272, 0.8531,\n",
       "          0.3745, 0.8296],\n",
       "         [0.7327, 0.9333, 0.6290, 0.9319, 0.8403, 0.9386, 0.4595, 0.3015,\n",
       "          0.1834, 0.2354],\n",
       "         [0.7939, 0.3729, 0.8950, 0.7705, 0.0322, 0.3933, 0.2047, 0.7017,\n",
       "          0.0232, 0.8769],\n",
       "         [0.0367, 0.6823, 0.2287, 0.7943, 0.6674, 0.3734, 0.3077, 0.3839,\n",
       "          0.0096, 0.2811]],\n",
       "\n",
       "        [[0.2449, 0.1406, 0.5033, 0.6830, 0.7837, 0.7431, 0.5814, 0.4511,\n",
       "          0.8850, 0.3344],\n",
       "         [0.2806, 0.9957, 0.0650, 0.5392, 0.2107, 0.5859, 0.4429, 0.3414,\n",
       "          0.9614, 0.9455],\n",
       "         [0.0512, 0.8538, 0.1843, 0.7981, 0.3456, 0.2357, 0.5257, 0.4352,\n",
       "          0.1256, 0.8128],\n",
       "         [0.0748, 0.5759, 0.2308, 0.3051, 0.0286, 0.1655, 0.9414, 0.1851,\n",
       "          0.2209, 0.1651]],\n",
       "\n",
       "        [[0.1542, 0.8504, 0.7882, 0.7466, 0.5299, 0.7172, 0.8075, 0.2960,\n",
       "          0.4832, 0.4731],\n",
       "         [0.8968, 0.5628, 0.9348, 0.5689, 0.5464, 0.2128, 0.4600, 0.5536,\n",
       "          0.5664, 0.4960],\n",
       "         [0.9587, 0.1933, 0.1780, 0.1439, 0.7586, 0.4737, 0.7456, 0.0403,\n",
       "          0.6641, 0.2561],\n",
       "         [0.2312, 0.7287, 0.6127, 0.5356, 0.6527, 0.2771, 0.9254, 0.3205,\n",
       "          0.0667, 0.3542]],\n",
       "\n",
       "        [[0.1394, 0.1670, 0.8722, 0.3283, 0.5246, 0.0373, 0.2065, 0.7477,\n",
       "          0.3913, 0.2509],\n",
       "         [0.9851, 0.0787, 0.8734, 0.7438, 0.6634, 0.9543, 0.6003, 0.0412,\n",
       "          0.8090, 0.8158],\n",
       "         [0.9899, 0.7047, 0.9626, 0.2676, 0.0887, 0.9637, 0.0066, 0.6357,\n",
       "          0.4018, 0.2179],\n",
       "         [0.1419, 0.4510, 0.2898, 0.0982, 0.9327, 0.5189, 0.0675, 0.3530,\n",
       "          0.6670, 0.5746]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first dimension\n",
    "sents_emb_keys[0]   # 0-th sentence, all words, all heads, all hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09db0840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_emb_keys[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d17b3",
   "metadata": {},
   "source": [
    "Q: what does [5, 4, 10] mean?\n",
    "\n",
    "A: for the 0-th sentence, it has [length = 5, number_of_heads = 4, hidden_size = 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447864cb",
   "metadata": {},
   "source": [
    "In other words, fixing the index of the first dimension of a four-dimensional tensor will result in a three-dimensional tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76139e11",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0183, 0.0201, 0.8848, 0.9264, 0.9478, 0.4991, 0.8272, 0.8531,\n",
       "          0.3745, 0.8296],\n",
       "         [0.7327, 0.9333, 0.6290, 0.9319, 0.8403, 0.9386, 0.4595, 0.3015,\n",
       "          0.1834, 0.2354],\n",
       "         [0.7939, 0.3729, 0.8950, 0.7705, 0.0322, 0.3933, 0.2047, 0.7017,\n",
       "          0.0232, 0.8769],\n",
       "         [0.0367, 0.6823, 0.2287, 0.7943, 0.6674, 0.3734, 0.3077, 0.3839,\n",
       "          0.0096, 0.2811]],\n",
       "\n",
       "        [[0.3183, 0.5212, 0.0634, 0.0529, 0.7391, 0.0951, 0.1299, 0.0450,\n",
       "          0.9903, 0.3069],\n",
       "         [0.1189, 0.5413, 0.8235, 0.2123, 0.8520, 0.0638, 0.1025, 0.1105,\n",
       "          0.3888, 0.0779],\n",
       "         [0.3216, 0.0270, 0.0798, 0.2138, 0.7519, 0.9796, 0.6267, 0.3120,\n",
       "          0.2358, 0.9861],\n",
       "         [0.7409, 0.0600, 0.8958, 0.3956, 0.2583, 0.9841, 0.7147, 0.5213,\n",
       "          0.7362, 0.2340]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_emb_keys[:, 1, :, :]  # All sentence, 1-st words, all heads, all hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9055b306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_emb_keys[:, 1, :, :].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33639a87",
   "metadata": {},
   "source": [
    "Q: what does [2, 4, 10] mean?\n",
    "\n",
    "A: when fixing the 1-st word, we have [batch_size = 2, number_of_heads = 4, hidden_size = 10]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc09d839",
   "metadata": {},
   "source": [
    "Similarlly we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef6c9770",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9654, 0.4685, 0.4361, 0.7805],\n",
       "         [0.0183, 0.7327, 0.7939, 0.0367],\n",
       "         [0.2449, 0.2806, 0.0512, 0.0748],\n",
       "         [0.1542, 0.8968, 0.9587, 0.2312],\n",
       "         [0.1394, 0.9851, 0.9899, 0.1419]],\n",
       "\n",
       "        [[0.3742, 0.0244, 0.0438, 0.7269],\n",
       "         [0.3183, 0.1189, 0.3216, 0.7409],\n",
       "         [0.2885, 0.3024, 0.8328, 0.0341],\n",
       "         [0.7774, 0.7344, 0.6158, 0.6605],\n",
       "         [0.1123, 0.1531, 0.3796, 0.2223]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_emb_keys[0, :, :, :] # 0-st sentence, all words, all heads, all hidden\n",
    "sents_emb_keys[:, 0, :, :] # all sentence, 0-th word, all heads, all hidden\n",
    "sents_emb_keys[:, :, 0, :] # all sentence, all words, 0-th head, all hidden\n",
    "sents_emb_keys[:, :, :, 0] # all sentence, all words, all heads, 0-th hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c979c4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 10])\n",
      "torch.Size([2, 4, 10])\n",
      "torch.Size([2, 5, 10])\n",
      "torch.Size([2, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "# Q: interpret what the following shape means: \n",
    "# A: --- YOUR ANSWER HERE ----  # sents_emb_keys = torch.rand([2, 5, 4, 10]) \n",
    "print(sents_emb_keys[0, :, :, :].size())\n",
    "print(sents_emb_keys[:, 0, :, :].size())\n",
    "print(sents_emb_keys[:, :, 0, :].size())\n",
    "print(sents_emb_keys[:, :, :, 0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab042d",
   "metadata": {},
   "source": [
    "The index can be further fixed at multiple dimensions, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "423dd542",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3179, 0.3308, 0.9132, 0.1591, 0.3336, 0.0413, 0.8887, 0.3742, 0.0330,\n",
      "         0.3727],\n",
      "        [0.1295, 0.3205, 0.5178, 0.5268, 0.7565, 0.5705, 0.2365, 0.8082, 0.8561,\n",
      "         0.6492],\n",
      "        [0.1867, 0.9840, 0.7758, 0.6262, 0.2417, 0.2887, 0.2965, 0.4177, 0.7826,\n",
      "         0.4871],\n",
      "        [0.6364, 0.6007, 0.5918, 0.4352, 0.0487, 0.6366, 0.1348, 0.8677, 0.2871,\n",
      "         0.1356]])\n",
      "tensor([0.1867, 0.9840, 0.7758, 0.6262, 0.2417, 0.2887, 0.2965, 0.4177, 0.7826,\n",
      "        0.4871])\n",
      "tensor([[0.1867, 0.9840, 0.7758, 0.6262, 0.2417, 0.2887, 0.2965, 0.4177, 0.7826,\n",
      "         0.4871],\n",
      "        [0.5468, 0.6296, 0.0364, 0.3594, 0.4154, 0.6182, 0.7581, 0.9338, 0.5036,\n",
      "         0.1573]])\n"
     ]
    }
   ],
   "source": [
    "print(sents_emb_keys[0, 1, :, :]) # 0-th sentence, 1-st word, all heads, all hidden\n",
    "print(sents_emb_keys[0, 1, 2, :]) # 0-th sentence, 1-st word, 2-nd head, all hidden\n",
    "print(sents_emb_keys[:, 1, 2, :]) # all sentences, 1-st word, 2-nd head, all hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234dee4",
   "metadata": {},
   "source": [
    "Looking at the shape of the resulting tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a73b891c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_emb_keys[:, 1, 2, :].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626211bd",
   "metadata": {},
   "source": [
    "[2, 10] means [batch_size = 2, hidden_size = 10] when fixing the 1-st word and the 2-nd head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9964b65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Q: interpret what the following shape means: \n",
    "# A: ---- YOUR ANSWER HERE ---\n",
    "print(sents_emb_keys[0, 1, :, :].size())\n",
    "print(sents_emb_keys[0, 1, 2, :].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22500ceb",
   "metadata": {},
   "source": [
    "### Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e1f3aa",
   "metadata": {},
   "source": [
    "Slicing takes a range of an index within any dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5c99bd9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.8729, 0.9025, 0.6452, 0.3510, 0.8613, 0.7899, 0.6674, 0.9150],\n",
       "          [0.5535, 0.6011, 0.5433, 0.1122, 0.1387, 0.7824, 0.3964, 0.5836],\n",
       "          [0.0614, 0.6729, 0.5840, 0.0016, 0.3817, 0.7127, 0.0014, 0.1786],\n",
       "          [0.9873, 0.1219, 0.6754, 0.9230, 0.8386, 0.8947, 0.1473, 0.8560]],\n",
       "\n",
       "         [[0.3308, 0.9132, 0.1591, 0.3336, 0.0413, 0.8887, 0.3742, 0.0330],\n",
       "          [0.3205, 0.5178, 0.5268, 0.7565, 0.5705, 0.2365, 0.8082, 0.8561],\n",
       "          [0.9840, 0.7758, 0.6262, 0.2417, 0.2887, 0.2965, 0.4177, 0.7826],\n",
       "          [0.6007, 0.5918, 0.4352, 0.0487, 0.6366, 0.1348, 0.8677, 0.2871]],\n",
       "\n",
       "         [[0.3594, 0.3975, 0.0518, 0.9698, 0.3253, 0.7577, 0.4650, 0.4150],\n",
       "          [0.9912, 0.1387, 0.1541, 0.1930, 0.5015, 0.4293, 0.3002, 0.4657],\n",
       "          [0.6919, 0.2448, 0.5146, 0.3169, 0.1226, 0.3471, 0.1492, 0.4953],\n",
       "          [0.2616, 0.2614, 0.6155, 0.7317, 0.2351, 0.5597, 0.1290, 0.3548]],\n",
       "\n",
       "         [[0.6291, 0.6374, 0.2383, 0.4704, 0.6025, 0.7852, 0.9046, 0.4481],\n",
       "          [0.9614, 0.7614, 0.9325, 0.5496, 0.3769, 0.4365, 0.7999, 0.4480],\n",
       "          [0.0044, 0.5574, 0.3041, 0.5696, 0.5299, 0.2368, 0.4774, 0.0051],\n",
       "          [0.2217, 0.4519, 0.1291, 0.3758, 0.5096, 0.4929, 0.7431, 0.8968]],\n",
       "\n",
       "         [[0.7394, 0.8725, 0.4858, 0.1086, 0.7844, 0.3104, 0.5699, 0.0870],\n",
       "          [0.8955, 0.0373, 0.8702, 0.9629, 0.6660, 0.2640, 0.5048, 0.6372],\n",
       "          [0.8986, 0.3639, 0.1536, 0.8419, 0.2226, 0.9963, 0.0507, 0.6431],\n",
       "          [0.3769, 0.7232, 0.1433, 0.5051, 0.2982, 0.4983, 0.7094, 0.6577]]],\n",
       "\n",
       "\n",
       "        [[[0.2559, 0.9162, 0.7375, 0.6582, 0.6377, 0.5439, 0.2733, 0.1401],\n",
       "          [0.0384, 0.0264, 0.1529, 0.9963, 0.0339, 0.8921, 0.0298, 0.7980],\n",
       "          [0.7555, 0.8420, 0.2294, 0.2386, 0.9277, 0.0665, 0.1536, 0.3404],\n",
       "          [0.8263, 0.8365, 0.4561, 0.7731, 0.9553, 0.1870, 0.9385, 0.7123]],\n",
       "\n",
       "         [[0.4046, 0.1233, 0.3721, 0.8163, 0.5609, 0.9449, 0.3421, 0.6740],\n",
       "          [0.4452, 0.7289, 0.3012, 0.6445, 0.5195, 0.0187, 0.9848, 0.5169],\n",
       "          [0.6296, 0.0364, 0.3594, 0.4154, 0.6182, 0.7581, 0.9338, 0.5036],\n",
       "          [0.3786, 0.6289, 0.0023, 0.0223, 0.7829, 0.8542, 0.2147, 0.4096]],\n",
       "\n",
       "         [[0.4712, 0.1777, 0.1419, 0.2479, 0.0469, 0.7636, 0.5502, 0.9655],\n",
       "          [0.1784, 0.0907, 0.3211, 0.1360, 0.0038, 0.0503, 0.9020, 0.9479],\n",
       "          [0.1742, 0.9005, 0.3474, 0.6063, 0.2215, 0.8284, 0.4906, 0.5795],\n",
       "          [0.4636, 0.8296, 0.5921, 0.2010, 0.0285, 0.5290, 0.8736, 0.2330]],\n",
       "\n",
       "         [[0.5907, 0.0101, 0.1147, 0.8250, 0.3939, 0.8492, 0.1400, 0.6998],\n",
       "          [0.1555, 0.2942, 0.2694, 0.1815, 0.8392, 0.7912, 0.6716, 0.9553],\n",
       "          [0.5587, 0.5181, 0.2583, 0.2951, 0.3660, 0.6668, 0.6533, 0.0087],\n",
       "          [0.1731, 0.9630, 0.4493, 0.1557, 0.7922, 0.4139, 0.8403, 0.8637]],\n",
       "\n",
       "         [[0.5008, 0.9164, 0.0555, 0.6972, 0.7832, 0.0674, 0.6843, 0.6759],\n",
       "          [0.4191, 0.9658, 0.8410, 0.1329, 0.7871, 0.8482, 0.5406, 0.9348],\n",
       "          [0.1591, 0.1167, 0.0050, 0.0480, 0.5409, 0.1943, 0.0017, 0.0714],\n",
       "          [0.6190, 0.7699, 0.1945, 0.1790, 0.0597, 0.2155, 0.6224, 0.6407]]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_emb_keys[:, 1:4, :, :] # all sentences, 1-3 words (right boundary 4 is not inclusive), all heads, all hidden\n",
    "sents_emb_keys[:, :4, :, :]  # all sentences, 0-3 words (when left boundary is unspecified, the default is 0), all heads, all hidden\n",
    "sents_emb_keys[:, 1:, :, :]  # all sentences, 1-last words (when right boundary is unspecified, the default is the last token), all heads, all hidden\n",
    "sents_emb_keys[:, :, 2:4:, :] # all sentences, all words, 2-3 heads, all hidden\n",
    "sents_emb_keys[:, :, :, 1:9] # all sentences, all words, all heads, 1-8 hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce3a99",
   "metadata": {},
   "source": [
    "You can also set a stepsize for slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0897272",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.8334, 0.8729, 0.9025, 0.6452, 0.3510, 0.8613, 0.7899, 0.6674,\n",
       "           0.9150, 0.7840],\n",
       "          [0.1847, 0.5535, 0.6011, 0.5433, 0.1122, 0.1387, 0.7824, 0.3964,\n",
       "           0.5836, 0.0032],\n",
       "          [0.3672, 0.0614, 0.6729, 0.5840, 0.0016, 0.3817, 0.7127, 0.0014,\n",
       "           0.1786, 0.9084],\n",
       "          [0.8757, 0.9873, 0.1219, 0.6754, 0.9230, 0.8386, 0.8947, 0.1473,\n",
       "           0.8560, 0.3940]],\n",
       "\n",
       "         [[0.4954, 0.3594, 0.3975, 0.0518, 0.9698, 0.3253, 0.7577, 0.4650,\n",
       "           0.4150, 0.4991],\n",
       "          [0.9952, 0.9912, 0.1387, 0.1541, 0.1930, 0.5015, 0.4293, 0.3002,\n",
       "           0.4657, 0.6610],\n",
       "          [0.5574, 0.6919, 0.2448, 0.5146, 0.3169, 0.1226, 0.3471, 0.1492,\n",
       "           0.4953, 0.1999],\n",
       "          [0.6792, 0.2616, 0.2614, 0.6155, 0.7317, 0.2351, 0.5597, 0.1290,\n",
       "           0.3548, 0.1472]],\n",
       "\n",
       "         [[0.6946, 0.7394, 0.8725, 0.4858, 0.1086, 0.7844, 0.3104, 0.5699,\n",
       "           0.0870, 0.3478],\n",
       "          [0.6840, 0.8955, 0.0373, 0.8702, 0.9629, 0.6660, 0.2640, 0.5048,\n",
       "           0.6372, 0.9915],\n",
       "          [0.3574, 0.8986, 0.3639, 0.1536, 0.8419, 0.2226, 0.9963, 0.0507,\n",
       "           0.6431, 0.3495],\n",
       "          [0.8522, 0.3769, 0.7232, 0.1433, 0.5051, 0.2982, 0.4983, 0.7094,\n",
       "           0.6577, 0.4977]]],\n",
       "\n",
       "\n",
       "        [[[0.2559, 0.2559, 0.9162, 0.7375, 0.6582, 0.6377, 0.5439, 0.2733,\n",
       "           0.1401, 0.1108],\n",
       "          [0.3211, 0.0384, 0.0264, 0.1529, 0.9963, 0.0339, 0.8921, 0.0298,\n",
       "           0.7980, 0.6448],\n",
       "          [0.2794, 0.7555, 0.8420, 0.2294, 0.2386, 0.9277, 0.0665, 0.1536,\n",
       "           0.3404, 0.6123],\n",
       "          [0.6528, 0.8263, 0.8365, 0.4561, 0.7731, 0.9553, 0.1870, 0.9385,\n",
       "           0.7123, 0.9064]],\n",
       "\n",
       "         [[0.4684, 0.4712, 0.1777, 0.1419, 0.2479, 0.0469, 0.7636, 0.5502,\n",
       "           0.9655, 0.5543],\n",
       "          [0.2101, 0.1784, 0.0907, 0.3211, 0.1360, 0.0038, 0.0503, 0.9020,\n",
       "           0.9479, 0.0620],\n",
       "          [0.8120, 0.1742, 0.9005, 0.3474, 0.6063, 0.2215, 0.8284, 0.4906,\n",
       "           0.5795, 0.8834],\n",
       "          [0.1671, 0.4636, 0.8296, 0.5921, 0.2010, 0.0285, 0.5290, 0.8736,\n",
       "           0.2330, 0.2883]],\n",
       "\n",
       "         [[0.9926, 0.5008, 0.9164, 0.0555, 0.6972, 0.7832, 0.0674, 0.6843,\n",
       "           0.6759, 0.5344],\n",
       "          [0.7981, 0.4191, 0.9658, 0.8410, 0.1329, 0.7871, 0.8482, 0.5406,\n",
       "           0.9348, 0.9902],\n",
       "          [0.0250, 0.1591, 0.1167, 0.0050, 0.0480, 0.5409, 0.1943, 0.0017,\n",
       "           0.0714, 0.3833],\n",
       "          [0.0891, 0.6190, 0.7699, 0.1945, 0.1790, 0.0597, 0.2155, 0.6224,\n",
       "           0.6407, 0.9017]]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_emb_keys[:, 0::2, :, :] # all sentence, 0, 2, 4 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19ef5668",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.3179, 0.3308, 0.9132, 0.1591, 0.3336, 0.0413, 0.8887, 0.3742,\n",
       "           0.0330, 0.3727],\n",
       "          [0.1295, 0.3205, 0.5178, 0.5268, 0.7565, 0.5705, 0.2365, 0.8082,\n",
       "           0.8561, 0.6492],\n",
       "          [0.1867, 0.9840, 0.7758, 0.6262, 0.2417, 0.2887, 0.2965, 0.4177,\n",
       "           0.7826, 0.4871],\n",
       "          [0.6364, 0.6007, 0.5918, 0.4352, 0.0487, 0.6366, 0.1348, 0.8677,\n",
       "           0.2871, 0.1356]],\n",
       "\n",
       "         [[0.7748, 0.6291, 0.6374, 0.2383, 0.4704, 0.6025, 0.7852, 0.9046,\n",
       "           0.4481, 0.4605],\n",
       "          [0.3760, 0.9614, 0.7614, 0.9325, 0.5496, 0.3769, 0.4365, 0.7999,\n",
       "           0.4480, 0.7807],\n",
       "          [0.8757, 0.0044, 0.5574, 0.3041, 0.5696, 0.5299, 0.2368, 0.4774,\n",
       "           0.0051, 0.7699],\n",
       "          [0.7108, 0.2217, 0.4519, 0.1291, 0.3758, 0.5096, 0.4929, 0.7431,\n",
       "           0.8968, 0.4357]]],\n",
       "\n",
       "\n",
       "        [[[0.2905, 0.4046, 0.1233, 0.3721, 0.8163, 0.5609, 0.9449, 0.3421,\n",
       "           0.6740, 0.0334],\n",
       "          [0.5053, 0.4452, 0.7289, 0.3012, 0.6445, 0.5195, 0.0187, 0.9848,\n",
       "           0.5169, 0.2875],\n",
       "          [0.5468, 0.6296, 0.0364, 0.3594, 0.4154, 0.6182, 0.7581, 0.9338,\n",
       "           0.5036, 0.1573],\n",
       "          [0.7732, 0.3786, 0.6289, 0.0023, 0.0223, 0.7829, 0.8542, 0.2147,\n",
       "           0.4096, 0.2016]],\n",
       "\n",
       "         [[0.2877, 0.5907, 0.0101, 0.1147, 0.8250, 0.3939, 0.8492, 0.1400,\n",
       "           0.6998, 0.5963],\n",
       "          [0.4889, 0.1555, 0.2942, 0.2694, 0.1815, 0.8392, 0.7912, 0.6716,\n",
       "           0.9553, 0.3265],\n",
       "          [0.9506, 0.5587, 0.5181, 0.2583, 0.2951, 0.3660, 0.6668, 0.6533,\n",
       "           0.0087, 0.0979],\n",
       "          [0.2865, 0.1731, 0.9630, 0.4493, 0.1557, 0.7922, 0.4139, 0.8403,\n",
       "           0.8637, 0.6278]]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_emb_keys[:, 1::2, :, :] # # all sentence, 1, 3 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65143706",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.8334, 0.8729, 0.9025, 0.6452, 0.3510, 0.8613, 0.7899, 0.6674,\n",
       "           0.9150, 0.7840],\n",
       "          [0.1847, 0.5535, 0.6011, 0.5433, 0.1122, 0.1387, 0.7824, 0.3964,\n",
       "           0.5836, 0.0032],\n",
       "          [0.3672, 0.0614, 0.6729, 0.5840, 0.0016, 0.3817, 0.7127, 0.0014,\n",
       "           0.1786, 0.9084],\n",
       "          [0.8757, 0.9873, 0.1219, 0.6754, 0.9230, 0.8386, 0.8947, 0.1473,\n",
       "           0.8560, 0.3940]],\n",
       "\n",
       "         [[0.4954, 0.3594, 0.3975, 0.0518, 0.9698, 0.3253, 0.7577, 0.4650,\n",
       "           0.4150, 0.4991],\n",
       "          [0.9952, 0.9912, 0.1387, 0.1541, 0.1930, 0.5015, 0.4293, 0.3002,\n",
       "           0.4657, 0.6610],\n",
       "          [0.5574, 0.6919, 0.2448, 0.5146, 0.3169, 0.1226, 0.3471, 0.1492,\n",
       "           0.4953, 0.1999],\n",
       "          [0.6792, 0.2616, 0.2614, 0.6155, 0.7317, 0.2351, 0.5597, 0.1290,\n",
       "           0.3548, 0.1472]]],\n",
       "\n",
       "\n",
       "        [[[0.2559, 0.2559, 0.9162, 0.7375, 0.6582, 0.6377, 0.5439, 0.2733,\n",
       "           0.1401, 0.1108],\n",
       "          [0.3211, 0.0384, 0.0264, 0.1529, 0.9963, 0.0339, 0.8921, 0.0298,\n",
       "           0.7980, 0.6448],\n",
       "          [0.2794, 0.7555, 0.8420, 0.2294, 0.2386, 0.9277, 0.0665, 0.1536,\n",
       "           0.3404, 0.6123],\n",
       "          [0.6528, 0.8263, 0.8365, 0.4561, 0.7731, 0.9553, 0.1870, 0.9385,\n",
       "           0.7123, 0.9064]],\n",
       "\n",
       "         [[0.4684, 0.4712, 0.1777, 0.1419, 0.2479, 0.0469, 0.7636, 0.5502,\n",
       "           0.9655, 0.5543],\n",
       "          [0.2101, 0.1784, 0.0907, 0.3211, 0.1360, 0.0038, 0.0503, 0.9020,\n",
       "           0.9479, 0.0620],\n",
       "          [0.8120, 0.1742, 0.9005, 0.3474, 0.6063, 0.2215, 0.8284, 0.4906,\n",
       "           0.5795, 0.8834],\n",
       "          [0.1671, 0.4636, 0.8296, 0.5921, 0.2010, 0.0285, 0.5290, 0.8736,\n",
       "           0.2330, 0.2883]]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_emb_keys[:, 0:4:2, :, :] # # all sentence, 0, 2 words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b6e2f7",
   "metadata": {},
   "source": [
    "As you can see, the slicing syntax is `some_tensor[start_index : end_index : step_size]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ec919e0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 10])\n",
      "torch.Size([2, 5, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Q: interpret what the following shape means:\n",
    "# A: ---- YOUR ANSWER HERE ----\n",
    "print(sents_emb_keys[:, 1:4, :, :].size())\n",
    "print(sents_emb_keys[:, :, 3:, :].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2623fae0",
   "metadata": {},
   "source": [
    "### Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986d9038",
   "metadata": {},
   "source": [
    "To concatenate two tensors within a specified dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "602d7b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = torch.tensor([[1, 2, 3]]) # size = [1, 3]\n",
    "sent_2 = torch.tensor([[4, 5, 6]]) # size = [1, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "458960e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# concatenate two sentences into a batch\n",
    "sents = torch.cat([sent_1, sent_2], dim=0)\n",
    "print(sents)\n",
    "print(sents.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "281bcbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "# concat the second sentence to the end of the first sentence\n",
    "sents1 = torch.cat([sent_1, sent_2], dim=1)\n",
    "print(sents1)\n",
    "print(sents1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bbfb94",
   "metadata": {},
   "source": [
    "### Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961e923d",
   "metadata": {},
   "source": [
    "Usually reshaping has two effects: (a) spliting tensors or (b) concatenating tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e957239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 3])\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# concatenating tensors\n",
    "print(sents)\n",
    "print(sents.size())\n",
    "sents_cat = sents.view(6)\n",
    "print(sents_cat)\n",
    "print(sents_cat.size()) # pay attention how the shape of the tensor changes from [2, 3] -> [6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e54c709e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 3])\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "# spliting tensors\n",
    "sents_split_1 = sents_cat.view(2, 3)\n",
    "print(sents_split_1)\n",
    "print(sents_split_1.size())\n",
    "sents_split_2 = sents_cat.view(3, 2)\n",
    "print(sents_split_2)\n",
    "print(sents_split_2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30332366",
   "metadata": {},
   "source": [
    "### Transposing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184c7f4c",
   "metadata": {},
   "source": [
    "Transpose the two dimensions of a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bbaa12b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "print(sents) # shape = [batch, length]\n",
    "sents_transposed = sents.transpose(0, 1)\n",
    "print(sents_transposed) # shape = [length, batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b85e24",
   "metadata": {},
   "source": [
    "### Caveat: pay close attention to the difference between transposing and reshaping\n",
    "\n",
    "Generally, reshaping does not change the order of the items within the tensor, while tranposing changes the order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7670de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([1, 4, 2, 5, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "# view does not change order\n",
    "print(sents.view(6))\n",
    "print(sents.view(2, 3))\n",
    "print(sents.view(3, 2))\n",
    "\n",
    "# transposing changes the order\n",
    "print(sents.transpose(0, 1))\n",
    "print(sents.transpose(0, 1).reshape(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf63b1",
   "metadata": {},
   "source": [
    "## Basic Tensor Computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34714806",
   "metadata": {},
   "source": [
    "### Tensor add scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59782a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3594, 0.2411, 0.6584, 0.6098, 0.4540],\n",
      "        [0.4520, 0.4716, 0.0068, 0.6186, 0.3309],\n",
      "        [0.8766, 0.3103, 0.1715, 0.1097, 0.3585]])\n",
      "tensor([[1.3594, 1.2411, 1.6584, 1.6098, 1.4540],\n",
      "        [1.4520, 1.4716, 1.0068, 1.6186, 1.3309],\n",
      "        [1.8766, 1.3103, 1.1715, 1.1097, 1.3585]])\n",
      "tensor([[-0.6406, -0.7589, -0.3416, -0.3902, -0.5460],\n",
      "        [-0.5480, -0.5284, -0.9932, -0.3814, -0.6691],\n",
      "        [-0.1234, -0.6897, -0.8285, -0.8903, -0.6415]])\n",
      "tensor([[0.7188, 0.4822, 1.3168, 1.2195, 0.9080],\n",
      "        [0.9039, 0.9433, 0.0136, 1.2373, 0.6618],\n",
      "        [1.7532, 0.6205, 0.3429, 0.2194, 0.7170]])\n",
      "tensor([[0.1797, 0.1205, 0.3292, 0.3049, 0.2270],\n",
      "        [0.2260, 0.2358, 0.0034, 0.3093, 0.1655],\n",
      "        [0.4383, 0.1551, 0.0857, 0.0549, 0.1793]])\n"
     ]
    }
   ],
   "source": [
    "# addition, differences, multiplication, division\n",
    "sent_emb = torch.rand(3, 5)    # [length = 5, hidden = 10] \n",
    "                                # we assume we work with one sentence embedding where there are 3 words and \n",
    "                                # each word is associated with an embedding vector with length 5\n",
    "print(sent_emb)\n",
    "\n",
    "# the following operations are applied to every entry within the tensor\n",
    "print(sent_emb + 1)\n",
    "print(sent_emb - 1)\n",
    "print(sent_emb * 2)\n",
    "print(sent_emb / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da213a",
   "metadata": {},
   "source": [
    "### Vector dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "609a3ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector dot-product\n",
    "word_1 = torch.rand(5)\n",
    "word_2 = torch.rand(5)\n",
    "\n",
    "prod = (word_1 * word_2).sum() # element-wise multiplication then sum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fcbbeddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_1 = torch.tensor([1, 2, 3])\n",
    "word_2 = torch.tensor([1, 1, 1])\n",
    "prod = (word_1 * word_2).sum()\n",
    "prod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef9d8b",
   "metadata": {},
   "source": [
    "### Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b2dfa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "# matrix mulplication\n",
    "M1 = torch.rand(2, 5)\n",
    "M2 = torch.rand(5, 10)\n",
    "\n",
    "prod = torch.matmul(M1, M2)\n",
    "print(prod.size()) # shape changes: [2, 5] * [5, 10] -> [2, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81278a8",
   "metadata": {},
   "source": [
    "## Alignment and broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a013f",
   "metadata": {},
   "source": [
    "### Rule 1: if the same shape, then element-wise computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b3493cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding two words, each word is a vector\n",
    "word_1 = torch.rand(5) # hidden_size = 5\n",
    "word_2 = torch.rand(5)\n",
    "result = word_1 + word_2 # result[i] = word_1[i] + word_2[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd75af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding two sentences, each sentence is a sequence of word vectors\n",
    "sent_1 = torch.rand(3, 5) # sent_length = 3, hidden_size = 5\n",
    "sent_2 = torch.rand(3, 5)\n",
    "result = sent_1 + sent_2 # result[i, j] = sent_1[i, j] + sent_2[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6749fee",
   "metadata": {},
   "source": [
    "### Rule 2: if different shape, first align, then element-wise computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6bfbaa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [2, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# Example 1. adding a single word to every word within a sentence\n",
    "word = torch.tensor([1, 2, 3])              # [hidden = 3]\n",
    "sent = torch.tensor([[0, 0, 0], \n",
    "                     [1, 1, 1]]) # [sent_length = 2, hidden = 3]\n",
    "\n",
    "result = word.view(1, 3) + sent # underlying process: \n",
    "                                # step 1, alignment\n",
    "                                # word.size() = [1, 3]\n",
    "                                #                |  |   alignment\n",
    "                                # sent.size() = [2, 3]\n",
    "                                #\n",
    "                                # step 2, repeat where dimension size is 1\n",
    "                                # word_repeat = word.repeat([2, 1]) -- 0th dim repeat twice, first dim repeat 1 time (= no repeat)\n",
    "                                #\n",
    "                                # step 3, align again\n",
    "                                # align again\n",
    "                                # word_repeat.size() = [2, 3]\n",
    "                                #                       |  |   alignmeng, note that the 0th dim of word is repeated\n",
    "                                # sent.size()        = [2, 3]\n",
    "                                #\n",
    "                                # step 4, element-wise addition. \n",
    "                                # result = word_repeat + sent\n",
    "print(result)  # this is basically broadcasing the given word to each word in the given sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df1bd33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2,  3],\n",
      "         [ 2,  3,  4],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [ 8,  9, 10],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Example 2. adding two words to two sentences, respectively\n",
    "words = torch.tensor([[1, 2, 3], [4, 5, 6]])                      # batch_size = 2, hidden_size = 3\n",
    "sents = torch.tensor([\n",
    "                        [[0, 0, 0], [1, 1, 1], [2, 2, 2]], \n",
    "                        [[3, 3, 3], [4, 4, 4], [5, 5, 5]],        # batch_size = 2, length = 2, hidden_size = 3\n",
    "                    ])\n",
    "\n",
    "# we aim to add words[0] to sents[0] and words[1] to sents[1]\n",
    "result = words.view(2, 1, 3) + sents    # underlying process: \n",
    "                                        # \n",
    "                                        # step 1, alignment\n",
    "                                        # words.size() = [2, 1, 3]   two words, each has hidden dim = 3\n",
    "                                        #                 |  |  |    alignment\n",
    "                                        # sents.size() = [2, 2, 3]   two sentences, each has two words, each word has hidden dim = 3\n",
    "                                        # \n",
    "                                        # step 2, repeat where dimension size is 1 for broadcasting\n",
    "                                        # repeat words along 1st dimension\n",
    "                                        # words_repeat = words.repeat([1, 2, 1]) -- 0th dim repeat 1 time, 1st dim repeat 2 times, 3rd dim repeat 1 time\n",
    "                                        # \n",
    "                                        # step 3, align again\n",
    "                                        # words_repeat.size() = [2, 2, 3]\n",
    "                                        #                        |  |  |   alignment, note that the 1st dim of words is repeated\n",
    "                                        # sents.size()        = [2, 2, 3]\n",
    "                                        #\n",
    "                                        # step 4, element-wise addition\n",
    "                                        # result = words_repeat + sents\n",
    "print(result)\n",
    "print(result.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7237da9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  5,  6],\n",
      "        [ 8, 10, 12],\n",
      "        [12, 15, 18]])\n"
     ]
    }
   ],
   "source": [
    "# Example 3, outer product\n",
    "# the outer product of two vectors is a matrix\n",
    "v1 = torch.tensor([1, 2, 3])\n",
    "v2 = torch.tensor([4, 5, 6])\n",
    "\n",
    "outer_prod = v1.view(3, 1) * v2.view(1, 3)  # underlying process: \n",
    "                                            # \n",
    "                                            # step 1, alignment\n",
    "                                            # v1.size() = [3, 1]  \n",
    "                                            #              |  |  alignment\n",
    "                                            # v2.size() = [1, 3] \n",
    "                                            # \n",
    "                                            # step 2, repeat where dimension size is 1\n",
    "                                            # repeat v1 along 1st dimension\n",
    "                                            # v1_repeat = v1.repeat([1, 3]) -- 0th dim repeat 1 time, 1st repeat 3 times\n",
    "                                            # repeat v2 along 0th dimension\n",
    "                                            # v2_repeat = v2.repeat([3, 1]) -- 0th dim repeat 3 times, 1st repeat 1 time\n",
    "                                            #\n",
    "                                            # step 3, align again and element-wise computation\n",
    "                                            # v1_repeat.size() = [3, 3]\n",
    "                                            #                     |  | \n",
    "                                            # v2_repeat.size() = [3, 3]\n",
    "                                            #\n",
    "                                            # step 4, elementwise multiplication\n",
    "                                            # outer_prod = v1_repeat * v2_repeat\n",
    "\n",
    "print(outer_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b0b331c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  5,  6],\n",
      "        [ 8, 10, 12],\n",
      "        [12, 15, 18]])\n",
      "tensor([[ 4,  8, 12],\n",
      "        [ 5, 10, 15],\n",
      "        [ 6, 12, 18]])\n"
     ]
    }
   ],
   "source": [
    "# Pay attention to the shape of the tensor, again\n",
    "v1 = torch.tensor([1, 2, 3])\n",
    "v2 = torch.tensor([4, 5, 6])\n",
    "\n",
    "# Q: what is the differences between the following two operations: \n",
    "# A: ---- YOUR ANSWER HERE ----\n",
    "result_1 = v1.view(3, 1) * v2.view(1, 3)\n",
    "result_2 = v1.view(1, 3) * v2.view(3, 1)\n",
    "print(result_1)\n",
    "print(result_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024c1b1e",
   "metadata": {},
   "source": [
    "# Section 2. Batchified Tensor Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fafb937",
   "metadata": {},
   "source": [
    "In this section, we will study how to compute with tensors whose first dimension is the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd19b3c1",
   "metadata": {},
   "source": [
    "## Batch matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1f9adfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 20])\n"
     ]
    }
   ],
   "source": [
    "# linear transform of the word embeddings of two sentences\n",
    "sents = torch.rand(2, 5, 10) # [batch_size = 2, length = 2, hidden_size= 10]\n",
    "weight = torch.rand(10, 20)  # we transform each word from a length-10 vector to a length-20 vector\n",
    "\n",
    "sents_transform = torch.matmul(sents, weight)   # underlying process:\n",
    "                                                # sents.size() = [2,  5, 10]\n",
    "                                                #                 |       |         alignment. \n",
    "                                                # view weight as [1,     10, 20]    repeat where dimension is 1\n",
    "                                                # matrix multiplication happens at the final two dimensions, [2, 5, 10] x [10, 20] -> [2, 5, 20]\n",
    "print(sents_transform.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a101bac",
   "metadata": {},
   "source": [
    "## Retrieving embedding vectors from an embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "07d13637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.Size([2, 5, 10])\n",
      "tensor([0.9432, 0.9382, 0.6470, 0.1222, 0.3370, 0.7655, 0.0559, 0.8464, 0.0360,\n",
      "        0.0308])\n",
      "tensor([0.9432, 0.9382, 0.6470, 0.1222, 0.3370, 0.7655, 0.0559, 0.8464, 0.0360,\n",
      "        0.0308])\n",
      "tensor([0.3585, 0.0922, 0.4751, 0.5944, 0.1077, 0.4041, 0.0765, 0.9159, 0.2264,\n",
      "        0.6065])\n",
      "tensor([0.3585, 0.0922, 0.4751, 0.5944, 0.1077, 0.4041, 0.0765, 0.9159, 0.2264,\n",
      "        0.6065])\n"
     ]
    }
   ],
   "source": [
    "# in deep learning practice, usually we start with sequences of word index\n",
    "sent = torch.tensor([[10, 25, 59, 77, 88],  # suppose [10, 25, 59, 77, 88] means = ['Oh', I', 'really', 'like', 'cats']\n",
    "                     [16, 29, 40, 56, 3]]   # suppose [16, 29, 40, 56, 3] means = ['Jack', 'does', 'not', 'have', 'dogs']\n",
    "                    )\n",
    "print(sent.size())\n",
    "\n",
    "# neural networks requires words being represented as embedding vectors, not index. So we store an embedding matrix for every word\n",
    "vocab_size = 100\n",
    "hidden_size = 10\n",
    "embedding_matrix = torch.rand([vocab_size, hidden_size])\n",
    "\n",
    "# converting index to embeddings consists of two steps:\n",
    "# 1. convert index representation to one-hot representation\n",
    "sent_one_hot = F.one_hot(sent, vocab_size).float()\n",
    "# 2. batch matrix multiplication between the one hot representation and the embedding matrix\n",
    "#    NOTE: when multiply a one-hot vector to a matrix, the one-hot vector essentially retrieves the corresponding row vector from the matrix\n",
    "sent_emb = torch.matmul(sent_one_hot, embedding_matrix) # underlying process:\n",
    "                                                        # sent_one_hot.size() =    [2,  5, 100]\n",
    "                                                        #                           |       |         alignment. \n",
    "                                                        # view embedding_matrix as [1,     100, 10]   repeat where dimension is 1\n",
    "                                                        # matrix multiplication happens at the final two dimensions, [2, 5, 100] x [100, 10] -> [2, 5, 10]\n",
    "                                                        # In this case, multiplying an one-hot vector to a matrix means to retrieve the corresponding \n",
    "                                                        # row vector from the matrix\n",
    "\n",
    "print(sent_emb.size())\n",
    "\n",
    "# the following show the embedding vector from word index 10 is retrieved for sent[0, 0]\n",
    "print(sent_emb[0, 0])\n",
    "print(embedding_matrix[10])\n",
    "\n",
    "# similarlly the embedding vector from word index 25 is retrieved for sent[0, 1]\n",
    "print(sent_emb[0, 1])\n",
    "print(embedding_matrix[25])\n",
    "\n",
    "# ---- YOUR TASK ----\n",
    "# verify how one-hot vector can retrieve the corresponding row vector from the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe10d0f1",
   "metadata": {},
   "source": [
    "## Linear transform of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d17dcea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 4, 20])\n"
     ]
    }
   ],
   "source": [
    "# linear transform of the word embeddings of two sentences where each sentence has 4 attention heads\n",
    "# this computation is performed in multi-head attention in CW2\n",
    "\n",
    "sents = torch.rand(2, 5, 4, 10) # [batch_size = 2, length = 2, number_of_heads = 4, hidden_size= 10]\n",
    "weight = torch.rand(10, 20)  # we transform each word from a length-10 vector to a length-20 vector\n",
    "\n",
    "sents_transform = torch.matmul(sents, weight)   # underlying process:\n",
    "                                                # sents.size() = [2,  5, 4, 10]\n",
    "                                                #                 |   |      |         alignment. \n",
    "                                                # view weight as [1,  1,    10, 20]    repeat where dimension is 1\n",
    "                                                # matrix multiplication happens at the final two dimensions, [2, 5, 4, 10] * [10, 20] -> [2, 5, 4, 20]\n",
    "print(sents_transform.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d4c31c",
   "metadata": {},
   "source": [
    "## Similarity: one single vector v.s. a batch of sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aabc9979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "# similarity: one single vector v.s. a batch of sentence representations\n",
    "query = torch.rand(10)\n",
    "sents = torch.rand(2, 5, 10)\n",
    "\n",
    "# suppose we would like to compute the dot-product (as a measure of similarity) between the query vector and all words within all sentences\n",
    "similarity = (query.view(1, 1, 10) * sents)\n",
    "print(similarity.size())  # similarity[i, j] means the vector dot-product between the query vector and the word j at sentence i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7103b059",
   "metadata": {},
   "source": [
    "## Similarity: a batch of vectors v.s. a batch of sentence  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2ad20bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "# similarity: a batch of vectors v.s. a batch of sentence representations \n",
    "query = torch.rand(2, 10) # pay attention to the differences to the previous case\n",
    "sents = torch.rand(2, 5, 10)\n",
    "\n",
    "similarity = (query.view(2, 1, 10) * sents).sum(dim=2)\n",
    "print(similarity.size())  # similarity[i, j] means the vector dot-product between the query[i] and the word j at sentence i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c5f1e",
   "metadata": {},
   "source": [
    "## Similarity: a sentence v.s. a batch of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c2474394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 5])\n",
      "torch.Size([2, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "# similarity: a sentence v.s. a batch of sentence representations\n",
    "sent_0 = torch.rand(4, 10)\n",
    "sents = torch.rand(2, 5, 10)\n",
    "\n",
    "# Q: what is the underlying process? hint: recall the previous align-and-broadcast\n",
    "# A: ---- YOUR ANSWER HERE ----\n",
    "similarity = (sent_0.view(1, 4, 1, 10) * sents.view(2, 1, 5, 10)).sum(dim=3) \n",
    "print(similarity.size()) # similarity[i, j, k] means the vector dot-product between sent_0[j] (j-th word) and sents[i, k] (i-th sent, k-th word)\n",
    "\n",
    "similarity = (sent_0.view(1, 1, 4, 10) * sents.view(2, 5, 1, 10)).sum(dim=3)\n",
    "print(similarity.size()) # similarity[i, j, k] means the vector dot-product between sent_0[k] (k-th word) and sents[i, j] (i-th sent, j-th word)\n",
    "\n",
    "# NOTE: pay attention two the differences of the shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b10abd",
   "metadata": {},
   "source": [
    "## Similarity: two batches of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "40a7cf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 6])\n",
      "torch.Size([2, 6, 5])\n"
     ]
    }
   ],
   "source": [
    "# similarity: two batches of sentences representations\n",
    "# this is usually happens in machine translation where we have a batch of sentences of source language, and another batch of sentences of target language\n",
    "source_sents = torch.rand(2, 5, 10) # [batch_size = 2, source_sent_length = 5, hidden_size = 10]\n",
    "target_sents = torch.rand(2, 6, 10) # [batch_size = 2, target_sent_length = 6, hidden_size = 10]\n",
    "\n",
    "# Q: what is the underlying process? hint: recall the previous align-and-broadcast\n",
    "# A: ---- YOUR ANSWER HERE ----\n",
    "similarity = (source_sents.view(2, 5, 1, 10) * target_sents.view(2, 1, 6, 10)).sum(dim = 3)\n",
    "print(similarity.size()) # similarity[i, j, k] means source_sents[i, j] v.s. target_sents[i, k]\n",
    "\n",
    "similarity = (source_sents.view(2, 1, 5, 10) * target_sents.view(2, 6, 1, 10)).sum(dim = 3)\n",
    "print(similarity.size()) # similarity[i, j, k] means source_sents[i, k] v.s. target_sents[i, j]\n",
    "# NOTE: pay attention two the differences of the shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e6e2f2",
   "metadata": {},
   "source": [
    "# Section 3. Masked Batch Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0766e5a",
   "metadata": {},
   "source": [
    "When putting sentences of difference length into a batch, common practice is to pad them into the same pre-specified maximum length with a special PAD token, then mask out computation involving the PAD token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92e8a05",
   "metadata": {},
   "source": [
    "## Basic Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "883d0e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a batch of sentences with different length\n",
    "PAD = 0\n",
    "sents = torch.tensor([[1, 2, 3, 0, 0], # length = 3, 2 PAD tokens,\n",
    "                      [3, 4, 0, 0, 0], # length = 2, 3 PAD tokens\n",
    "                      [5, 6, 7, 8, 9]] # length = 5, no PAD token\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3577eb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# create a batch of mask vectors\n",
    "mask = (sents != PAD).float()\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4ca32d",
   "metadata": {},
   "source": [
    "## Masked Batchified Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a39fa5",
   "metadata": {},
   "source": [
    "Now we repeat the computation of the previous section. But this time, we mask out the computation involving the PAD tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abd5dbd",
   "metadata": {},
   "source": [
    "## Similarity: one single vector vs. a batch of sentence + mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6451aa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.5105, 2.2764, 2.8527, 0.0000, 0.0000],\n",
      "        [2.4761, 1.7679, 0.0000, 0.0000, 0.0000],\n",
      "        [4.0252, 2.5680, 2.1620, 2.9131, 4.3537]])\n",
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# one single vector v.s. a batch of sentence representations + a batch of mask\n",
    "query = torch.rand(10)\n",
    "sents = torch.rand(3, 5, 10)\n",
    "\n",
    "# suppose we would like to compute the dot-product (as a measure of similarity) between the query vector and all words within all sentences\n",
    "similarity = (query.view(1, 1, 10) * sents * mask.view(3, 5, 1)).sum(dim=2) # query.size = [1, 1, 10]\n",
    "                                                                            # sents.size = [3, 5, 10]\n",
    "                                                                            # mask.size  = [3, 5, 1]\n",
    "                                                                            # recall the align-then-broadcast introduced in the previous section\n",
    "print(similarity)\n",
    "print(similarity.size())  # similarity[i, j] means the vector dot-product between the query vector and the word j at sentence i\n",
    "                          # similarity scores for PAD tokens are masked to be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c0b88",
   "metadata": {},
   "source": [
    "## Similarity: a sentence vs. a batch of sentence + mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "546e37c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.3298, 1.4957, 1.2587, 0.0000, 0.0000],\n",
      "         [2.2465, 2.6633, 2.3970, 0.0000, 0.0000],\n",
      "         [1.5583, 1.8176, 1.7344, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[1.3758, 1.7666, 0.0000, 0.0000, 0.0000],\n",
      "         [2.2330, 2.6583, 0.0000, 0.0000, 0.0000],\n",
      "         [1.5869, 1.9688, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[2.1724, 1.1920, 1.0303, 1.3125, 1.0763],\n",
      "         [3.1026, 1.9577, 2.0134, 1.7060, 2.1650],\n",
      "         [2.9045, 1.9501, 1.5935, 1.9299, 1.8313],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "torch.Size([3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "# similarity: a sentence v.s. a batch of sentence representations\n",
    "sent_0 = torch.rand(4, 10)\n",
    "sent_0_mask = torch.tensor([1, 1, 1, 0]) # we assume the length of sent_0 is 3\n",
    "\n",
    "sents = torch.rand(3, 5, 10)\n",
    "\n",
    "# Q: what is the underlying process? hint: recall the previous align-and-broadcast\n",
    "# A: ---- YOUR ANSWER HERE ----\n",
    "#\n",
    "# hint: first mask out sent_0\n",
    "#       sent_0.size        = [1, 4, 1, 10]\n",
    "#       sent_0_mask.size   = [1, 4, 1, 1]\n",
    "#       sent_0_masked.size = [1, 4, 1, 10]\n",
    "#\n",
    "#       then mask out sents\n",
    "#       sents.size         = [3, 1, 5, 10]\n",
    "#       mask.size          = [3, 1, 5, 1 ]\n",
    "#       sents_masked.size  = [3, 1, 5, 10]\n",
    "#\n",
    "#       finally computed similarity score for the two masked tensors\n",
    "#       sent_0_masked.size = [1, 4, 1, 10]\n",
    "#       sents_masked.size  = [3, 1, 5, 10]\n",
    "#       similarity.size    = [3, 4, 5]\n",
    "\n",
    "similarity = (sent_0.view(1, 4, 1, 10) * sent_0_mask.view(1, 4, 1, 1) * sents.view(3, 1, 5, 10) * mask.view(3, 1, 5, 1)).sum(dim=3) \n",
    "\n",
    "print(similarity)\n",
    "print(similarity.size()) # similarity[i, j, k] means the vector dot-product between sent_0[j] (j-th word) and sents[i, k] (i-th sent, k-th word)\n",
    "                         # similarity scores for PAD tokens are masked to be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34618835",
   "metadata": {},
   "source": [
    "## Similarity: two batches of masked sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "82ee55b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2.5672, 2.6088, 2.5834, 0.0000, 0.0000, 0.0000],\n",
      "         [2.0834, 1.5814, 2.3227, 0.0000, 0.0000, 0.0000],\n",
      "         [2.5505, 2.8746, 2.9579, 0.0000, 0.0000, 0.0000],\n",
      "         [2.6716, 2.0214, 2.7910, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[2.0494, 2.5466, 2.8356, 2.0703, 2.9785, 3.8640],\n",
      "         [0.9491, 1.5451, 1.6202, 1.1407, 1.5279, 1.9398],\n",
      "         [3.0602, 2.4551, 3.5492, 2.8821, 3.5459, 4.9329],\n",
      "         [2.5598, 2.3238, 2.8777, 2.4559, 2.8768, 4.1201],\n",
      "         [2.0323, 1.8204, 2.0931, 2.0048, 1.5199, 2.5985]]])\n",
      "torch.Size([2, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# similarity: two batches of sentences representations\n",
    "# this is usually happens in machine translation where we have a batch of sentences of source language, and another batch of sentences of target language\n",
    "# here additionaly, we assume each sentence has its own mask \n",
    "source_sents = torch.rand(2, 5, 10) # [batch_size = 2, source_sent_length = 5, hidden_size = 10]\n",
    "source_mask  = torch.tensor([[1, 1, 1, 1, 0], # length = 4\n",
    "                             [1, 1, 1, 1, 1]] # length = 5\n",
    "                           )\n",
    "target_sents = torch.rand(2, 6, 10) # [batch_size = 2, target_sent_length = 6, hidden_size = 10]\n",
    "target_mask  = torch.tensor([[1, 1, 1, 0, 0, 0], # length = 3\n",
    "                             [1, 1, 1, 1, 1, 1]] # length = 6\n",
    "                           )\n",
    "\n",
    "# Q: what is the underlying process? hint: recall the previous align-and-broadcast-and-mask\n",
    "# A: ---- YOUR ANSWER HERE ----\n",
    "similarity = (source_sents.view(2, 5, 1, 10) * source_mask.view(2, 5, 1, 1) * target_sents.view(2, 1, 6, 10) * target_mask.view(2, 1, 6, 1)).sum(dim = 3)\n",
    "print(similarity)\n",
    "print(similarity.size()) # similarity[i, j, k] means source_sents[i, j] v.s. target_sents[i, k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fb929afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1],\n",
       "          [ 2],\n",
       "          [ 3],\n",
       "          [ 4]]],\n",
       "\n",
       "\n",
       "        [[[ 5],\n",
       "          [ 6],\n",
       "          [ 7],\n",
       "          [ 8]]],\n",
       "\n",
       "\n",
       "        [[[ 9],\n",
       "          [10],\n",
       "          [11],\n",
       "          [12]]]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = torch.tensor([[1, 2, 3, 4, 5, 6],\n",
    "                    [7, 8, 9, 10, 11, 12]])\n",
    "mat.view(3, 1, 4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ac733b",
   "metadata": {},
   "source": [
    "# Section 4. Final Application: Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e158ab",
   "metadata": {},
   "source": [
    "Finally, we compute the per-word negative log likelihood of a neural language model that puts everything that we have learned together.\n",
    "\n",
    "The per-word NLL for a sentence $x_1, ..., x_T$ is:\n",
    "\n",
    "$$\n",
    "- \\frac{1}{T} \\sum_t \\log p(x_t | x_{1:t-1})\n",
    "$$\n",
    "\n",
    "which is the average of word likelihood over sentence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5992bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start with the final layer of a neural language model which output a tensor named logits (logits means the vector before putting into softmax):\n",
    "logits = torch.rand(2, 5, 100)  # [batch_size = 2, max_sent_len = 5, vocab_size = 100]\n",
    "                                # in practice this should be the output of the final layer of a neural language model. Here we just simulate it as random tensor\n",
    "vocab_size = 100 # in practice the vocabulary size is usually >10K, here for demonstration we simplify it to 100\n",
    "\n",
    "# putting the logits into a softmax function gives the probability of each word within the vocabulary\n",
    "probs = F.softmax(logits, dim=2)\n",
    "\n",
    "# negative log likelihood requires the log of probability\n",
    "log_probs = F.log_softmax(logits, dim=2)\n",
    "\n",
    "# a language model aims to maximize the probability of each word in every sentence, but the PAD token should be excluded\n",
    "target_sent = torch.tensor([[10, 25, 59, 77, 0],   # length = 4, suppose [10, 25, 59, 77, 0] means = ['I', 'really', 'like', 'cats', 'PAD']\n",
    "                            [16, 29, 40, 56, 3]]   # length = 5, suppose [16, 29, 40, 56, 3] means = ['Jack', 'does', 'not', 'have', 'dogs']\n",
    "                          )\n",
    "\n",
    "# transform the index representation to the one-hot representation\n",
    "target_one_hot = F.one_hot(target_sent, vocab_size)\n",
    "\n",
    "# create mask for the target sentence\n",
    "mask = (target_sent != 0).float()\n",
    "\n",
    "# the following computation computes the negative log likelihood\n",
    "# ---- YOUR TASK ----\n",
    "# 1. annotate the shape of every tensor in the following computation\n",
    "# 2. explain every computation step\n",
    "negative_log_likelihood = (((-target_one_hot * log_probs).sum(dim=2) * mask).sum(dim=1) / mask.sum(dim=1)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad34e5f",
   "metadata": {},
   "source": [
    "# Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5e8a4e",
   "metadata": {},
   "source": [
    "Up to this stage, you have already suffered from endless annotation of shape of tensor. Is there a way that one can write code which is self-explanatory, rather than requires shape annotation at every single line? \n",
    "\n",
    "The answer is yes, and the solution is [einsum](https://rockt.github.io/2018/04/30/einsum) and [einops](https://einops.rocks/). You are encouraged to find them out yourself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
